from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
import google.generativeai as genai
import os
from dotenv import load_dotenv
import markdown
import pandas as pd
import json
import csv
import concurrent.futures
import random
import time

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

app = Flask(__name__)
CORS(app)

# ====== ì „ì—­ ë³€ìˆ˜ ======
user_setups = {}  # ì‚¬ìš©ì ì„¤ì • ì €ì¥
response_cache = {}  # ì‘ë‹µ ìºì‹œ

# ====== ìƒìˆ˜ ======
GEN_MAX_OUTPUT_TOKENS = 4096
GEN_TIMEOUT_SEC = 180
MD_TIMEOUT_SEC = 10
RETRY_MAX = 3
RETRY_BASE = 2
RETRY_JITTER = 1

# ====== RAG ì •ì±… ìƒìˆ˜ ======
K_TOTAL = 12  # í›„ë³´ ë¬¸ì„œ ì´ëŸ‰
K_ANSWER = 6  # ìµœì¢… ìŠ¤ë‹ˆí« ìˆ˜
SNIPPET_MIN_LENGTH = 450
SNIPPET_MAX_LENGTH = 700
DUPLICATE_THRESHOLD = 0.7  # Jaccard ìœ ì‚¬ë„ ì„ê³„ê°’
RECENCY_BUFFER_DAYS = 90  # ìµœì‹ ì„± ë²„í¼ (ì¼)

# RAG ì •ì±… ë§¤í•‘ (ë¼ìš°íŒ…ë³„ ì¿¼í„° ë° ê°€ì¤‘ì¹˜)
RAG_POLICY = {
    "default": {
        "quota": {"card": 6, "bizcsv": 3, "did": 1, "event": 2},
        "w": {"card": 1.0, "bizcsv": 0.85, "did": 0.9, "event": 0.8}
    },
    "trend": {
        "quota": {"card": 4, "bizcsv": 2, "did": 1, "event": 5},
        "w": {"card": 1.0, "bizcsv": 0.85, "did": 0.9, "event": 1.0}
    },
    "retention": {
        "quota": {"card": 5, "bizcsv": 3, "did": 2, "event": 2},
        "w": {"card": 1.0, "bizcsv": 0.85, "did": 1.0, "event": 0.8}
    },
    "diagnosis": {
        "quota": {"card": 5, "bizcsv": 4, "did": 2, "event": 1},
        "w": {"card": 1.0, "bizcsv": 0.9, "did": 0.95, "event": 0.6}
    },
    "loyalty": {
        "quota": {"card": 5, "bizcsv": 4, "did": 1, "event": 2},
        "w": {"card": 1.0, "bizcsv": 0.9, "did": 0.9, "event": 0.8}
    },
    "channel": {
        "quota": {"card": 5, "bizcsv": 4, "did": 1, "event": 2},
        "w": {"card": 1.0, "bizcsv": 0.9, "did": 0.85, "event": 0.85}
    }
}

# ì „ë¬¸ê°€ ì—­í•  ë§¤í•‘
EXPERT_ROLES = {
    "trend": "íŠ¸ë Œë“œ ë¶„ì„ ì „ë¬¸ê°€",
    "retention": "ê³ ê° ìœ ì§€ ì „ë¬¸ê°€", 
    "diagnosis": "ë¬¸ì œ ì§„ë‹¨ ì „ë¬¸ê°€",
    "loyalty": "ê³ ê° ì¶©ì„±ë„ ì „ë¬¸ê°€",
    "channel": "ë§ˆì¼€íŒ… ì±„ë„ ì „ë¬¸ê°€",
    "default": "ì„±ë™êµ¬ ë§ˆì¼€íŒ… ì „ë¬¸ê°€"
}

# ====== RAG ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ======
def jaccard_similarity(text1, text2):
    """Jaccard ìœ ì‚¬ë„ ê³„ì‚°"""
    words1 = set(text1.lower().split())
    words2 = set(text2.lower().split())
    intersection = len(words1.intersection(words2))
    union = len(words1.union(words2))
    return intersection / union if union > 0 else 0

def simple_bm25_score(text, query_terms):
    """ê°„ë‹¨í•œ BM25 ìŠ¤ì½”ì–´ ê³„ì‚°"""
    text_lower = text.lower()
    score = 0
    for term in query_terms:
        count = text_lower.count(term.lower())
        if count > 0:
            score += count * 1.2  # ê°„ë‹¨í•œ ê°€ì¤‘ì¹˜
    return score

def entity_match_score(text, location="", industry="", task_keywords=None):
    """ì—”í‹°í‹° ë§¤ì¹­ ìŠ¤ì½”ì–´ ê³„ì‚°"""
    score = 0
    text_lower = text.lower()
    
    # ì§€ì—­/ì—…ì¢… ì¼ì¹˜
    if location and location.lower() in text_lower:
        score += 0.5
    if industry and industry.lower() in text_lower:
        score += 0.5
    if location and industry and location.lower() in text_lower and industry.lower() in text_lower:
        score += 0.2  # ë‘˜ ë‹¤ ì¼ì¹˜ ì‹œ ì¶”ê°€ ë³´ë„ˆìŠ¤
    
    # íƒœìŠ¤í¬ í‚¤ì›Œë“œ ë§¤ì¹­
    if task_keywords:
        for keyword in task_keywords:
            if keyword.lower() in text_lower:
                score += 0.1
        score = min(score, 0.3)  # ìƒí•œ 0.3
    
    return score

def recency_score(text, source_type, today=None):
    """ìµœì‹ ì„± ìŠ¤ì½”ì–´ ê³„ì‚°"""
    if not today:
        from datetime import datetime
        today = datetime.now()
    
    score = 0
    if source_type == "event":
        # ì´ë²¤íŠ¸ëŠ” ë‚ ì§œ ì •ë³´ ì¶”ì¶œí•˜ì—¬ ìµœì‹ ì„± ê³„ì‚°
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë‚ ì§œ íŒŒì‹± ë¡œì§ í•„ìš”
        score = 0.5  # ê¸°ë³¸ê°’
    else:
        # ë‹¤ë¥¸ ì†ŒìŠ¤ëŠ” ë‚®ì€ ìµœì‹ ì„± ì˜í–¥
        score = 0.1
    
    return score

def numerics_score(text):
    """ìˆ˜ì¹˜ í¬í•¨ ìŠ¤ì½”ì–´ ê³„ì‚°"""
    import re
    # í¼ì„¼íŠ¸, ìˆ«ì, ì‹œê°„ëŒ€ íŒ¨í„´ ë§¤ì¹­
    patterns = [
        r'\d+%',  # í¼ì„¼íŠ¸
        r'\d+\.\d+',  # ì†Œìˆ˜ì 
        r'\d+:\d+',  # ì‹œê°„
        r'\d+-\d+ì‹œ'  # ì‹œê°„ëŒ€
    ]
    
    score = 0
    for pattern in patterns:
        matches = re.findall(pattern, text)
        score += len(matches) * 0.05
    
    return min(score, 0.2)  # ìƒí•œ 0.2

def diversity_boost_score(text, picked_texts):
    """ë‹¤ì–‘ì„± ë¶€ìŠ¤íŠ¸ ìŠ¤ì½”ì–´ ê³„ì‚°"""
    if not picked_texts:
        return 0
    
    # ë™ì¼ íŒŒì¼ ì—°ì† ê³¼ë°€ ì‹œ ê°ì 
    penalty = 0
    for picked in picked_texts[-2:]:  # ìµœê·¼ 2ê°œì™€ ë¹„êµ
        if jaccard_similarity(text, picked) > 0.8:
            penalty += 0.1
    
    return -penalty

def calculate_final_score(item, source_type, policy_weights, picked_items=None):
    """ìµœì¢… ìŠ¤ì½”ì–´ ê³„ì‚°"""
    text = item.get('content', '')
    location = item.get('location', '')
    industry = item.get('industry', '')
    
    # ê° ì»´í¬ë„ŒíŠ¸ ìŠ¤ì½”ì–´ ê³„ì‚°
    bm25_score = simple_bm25_score(text, [location, industry]) if location or industry else 0.5
    entity_score = entity_match_score(text, location, industry)
    recency_score_val = recency_score(text, source_type)
    source_weight = policy_weights.get(source_type, 1.0)
    numerics_score_val = numerics_score(text)
    diversity_score = diversity_boost_score(text, [p.get('content', '') for p in picked_items or []])
    
    # ê°€ì¤‘ í•©ê³„
    final_score = (
        0.35 * bm25_score +
        0.20 * entity_score +
        0.15 * recency_score_val +
        0.15 * source_weight +
        0.10 * numerics_score_val +
        0.05 * diversity_score
    )
    
    return final_score

def truncate_with_sentence(text, max_length):
    """ë¬¸ì¥ ê²½ê³„ë¥¼ ìœ ì§€í•˜ë©° í…ìŠ¤íŠ¸ ìë¥´ê¸°"""
    if len(text) <= max_length:
        return text
    
    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìë¥´ê¸°
    sentences = text.split('. ')
    result = ""
    for sentence in sentences:
        if len(result + sentence + '. ') <= max_length:
            result += sentence + '. '
        else:
            break
    
    return result.strip()

def format_citation(source_type, source_file, line_num=None):
    """ì¶œì²˜ í‘œê¸° í¬ë§· ìƒì„±"""
    citation_map = {
        "card": f"ì‹ í•œì¹´ë“œë¶„ì„.jsonl#{line_num or 'unknown'}",
        "bizcsv": f"ì—…ì¢…ë³„ë°ì´í„°/{source_file}#row{line_num or 'unknown'}",
        "did": f"did.csv#row{line_num or 'unknown'}",
        "event": f"{source_file}#row{line_num or 'unknown'}"
    }
    return f"(ì¶œì²˜: {citation_map.get(source_type, source_file)})"

# ====== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ======
def safe_markdown(text):
    """ì•ˆì „í•œ ë§ˆí¬ë‹¤ìš´ ë³€í™˜"""
    try:
        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as ex:
            f = ex.submit(markdown.markdown, text, extensions=['extra'])
            try:
                return f.result(timeout=MD_TIMEOUT_SEC)
            except Exception:
                return text
    except Exception:
        return text

def is_overloaded_error(msg):
    """ì˜¤ë²„ë¡œë“œ ì˜¤ë¥˜ ê°ì§€"""
    overloaded_keywords = ['overloaded', 'quota', 'limit', 'rate']
    return any(keyword in msg.lower() for keyword in overloaded_keywords)

def get_model():
    """ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°"""
    if not os.getenv('GOOGLE_API_KEY'):
        print("âš ï¸ ê²½ê³ : GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None
    else:
        print("âœ… Google API Keyê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
        genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))
        model = genai.GenerativeModel('gemini-2.5-flash')
        print("ğŸ¤– Gemini ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        return model

def load_calendar_events():
    """
    ë‹¬ë ¥ ì´ë²¤íŠ¸ ë¡œë“œ(ë‘ CSV í†µí•©)
    - ì»¬ëŸ¼ëª…ì€ ì§ˆë¬¸ì— ì˜¬ë ¤ì£¼ì‹  í—¤ë”ì™€ ë™ì¼í•˜ê²Œ ì‚¬ìš©
    - ë‚ ì§œ 'YYYY.M.D' â†’ ISO8601
    - endëŠ” exclusiveë¡œ +1ì¼
    - ì¸ì½”ë”© ìë™ íŒë³„
    - ì¤‘ë³µ Event_ID ì œê±°
    """
    from datetime import datetime, timedelta
    
    def _read_csv_any(path):
        """UTF-8(BOM) â†’ CP949 â†’ EUC-KR ìˆœìœ¼ë¡œ ì‹œë„"""
        for enc in ("utf-8-sig", "utf-8", "cp949", "euc-kr"):
            try:
                df = pd.read_csv(path, encoding=enc)
                return df
            except Exception:
                continue
        return None

    def _parse_dot_date(s: str) -> str:
        """
        '2023.1.9' â†’ '2023-01-09'
        ê³µë°±/None/ë¹ˆê°’ ì•ˆì „ ì²˜ë¦¬
        """
        if not s or not str(s).strip():
            return ""
        s = str(s).strip().rstrip(".")            # '2023.1.9.' ê°™ì€ ê¼¬ë¦¬ ì  ì œê±°
        try:
            dt = datetime.strptime(s, "%Y.%m.%d")
            return dt.strftime("%Y-%m-%d")
        except Exception:
            # í˜¹ì‹œ '2023.01.09' ì²˜ëŸ¼ 0íŒ¨ë”©ì´ ìˆì–´ë„ ìœ„ í¬ë§·ì´ ì²˜ë¦¬í•˜ë‹ˆ ì—¬ê¸°ë¡œ ì˜ ì•ˆì˜´.
            # ì˜ˆì™¸ ì‹œ ì›ë¬¸ ë°˜í™˜(ë””ë²„ê¹…ì„ ìœ„í•´)
            return s

    def _norm_row(row: dict, src: str, idx: int) -> dict:
        """CSV í•œ í–‰ì„ ì´ë²¤íŠ¸ ê°ì²´ë¡œ ì •ê·œí™”"""
        start = _parse_dot_date(row.get("Start_Date", ""))
        end_inclusive = _parse_dot_date(row.get("End_Date", ""))

        # FullCalendar ë“±ì€ endê°€ exclusiveì´ë¯€ë¡œ, ì¢…ë£Œì¼ì´ ìˆìœ¼ë©´ í•˜ë£¨ +1
        end_exclusive = ""
        if end_inclusive:
            try:
                end_exclusive = (datetime.strptime(end_inclusive, "%Y-%m-%d") + timedelta(days=1)).strftime("%Y-%m-%d")
            except Exception:
                end_exclusive = end_inclusive  # ì‹¤íŒ¨ ì‹œë¼ë„ ê·¸ëŒ€ë¡œ

        eid = row.get("Event_ID") or f"{src}_{idx}"
        title = row.get("Event_Name", "").strip()
        evtype = row.get("Event_Type", "").strip()
        location = row.get("Location_Address", "") or ""
        audience = row.get("Target_Audience", "") or ""
        district = row.get("Associated_District", "") or ""
        desc = row.get("Event_Description", "") or ""

        return {
            "id": str(eid),
            "title": title,
            "start": start,
            "end": end_exclusive,      # â† exclusiveë¡œ ì „ë‹¬
            "allDay": True,            # ë‚ ì§œ ë‹¨ìœ„ ì´ë²¤íŠ¸ ê°€ì •
            "type": evtype,
            "location": location,
            "audience": audience,
            "district": district,
            "description": desc,
            "source": src,
        }

    events = []
    seen_ids = set()

    files = [
        ("ê³µí†µì´ë²¤íŠ¸", os.path.join(app.root_path, "documents", "raw", "ì„±ë™êµ¬ ê³µí†µ_í•œì–‘ëŒ€_í¥í–‰ì˜í™” ì´ë²¤íŠ¸ DB.csv")),
        ("ì„±ìˆ˜íŒì—…", os.path.join(app.root_path, "documents", "raw", "ì„±ìˆ˜ íŒì—… ìµœì¢….csv")),
    ]

    for label, path in files:
        if not os.path.exists(path):
            continue
        df = _read_csv_any(path)
        if df is None:
            print(f"[ìº˜ë¦°ë”] CSV ì¸ì½”ë”© ì‹¤íŒ¨: {path}")
            continue

        # ì—´ ì´ë¦„ ê³µë°± ì œê±°/í‘œì¤€í™”(ì—‘ì…€ ì €ì¥ ì‹œ ê³µë°±ì´ ë¼ëŠ” ê²½ìš° ëŒ€ë¹„)
        df.columns = [str(c).strip() for c in df.columns]

        for i, row in df.iterrows():
            try:
                ev = _norm_row(row, label, i)
                # startê°€ ë¹„ë©´ ìŠ¤í‚µ
                if not ev["start"]:
                    continue
                # ì¤‘ë³µ ì œê±°(Event_ID ê¸°ì¤€)
                if ev["id"] in seen_ids:
                    continue
                seen_ids.add(ev["id"])
                events.append(ev)
            except Exception as e:
                print(f"[ìº˜ë¦°ë”] í–‰ íŒŒì‹± ì‹¤íŒ¨ {label}#{i}: {e}")

    print(f"ğŸ“… ì´ {len(events)}ê°œ ì´ë²¤íŠ¸ ë¡œë“œ ì™„ë£Œ")
    return events

def load_shinhan_data():
    """ì‹ í•œì¹´ë“œ ë°ì´í„° ë¡œë“œ"""
    shinhan_data = []
    try:
        shinhan_file = os.path.join(app.root_path, 'documents', 'raw', 'ì‹ í•œì¹´ë“œë¶„ì„.jsonl')
        if os.path.exists(shinhan_file):
            with open(shinhan_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    try:
                        data = json.loads(line.strip())
                        data['line_num'] = line_num
                        data['source_type'] = 'card'
                        shinhan_data.append(data)
                    except json.JSONDecodeError:
                        continue
    except Exception as e:
        print(f"Error loading Shinhan data: {e}")
    
    return shinhan_data

def search_card_jsonl(query, location="", industry="", top=18):
    """ì‹ í•œì¹´ë“œ JSONL ë°ì´í„° ê²€ìƒ‰"""
    shinhan_data = load_shinhan_data()
    results = []
    
    for item in shinhan_data:
        content = str(item.get('content', ''))
        relevance = 0
        
        # ì¿¼ë¦¬ ë§¤ì¹­
        if query.lower() in content.lower():
            relevance += 2
        
        # ì§€ì—­/ì—…ì¢… ë§¤ì¹­
        if location and location.lower() in content.lower():
            relevance += 1
        if industry and industry.lower() in content.lower():
            relevance += 1
        
        if relevance > 0:
            results.append({
                'content': content,
                'source_type': 'card',
                'source_file': 'ì‹ í•œì¹´ë“œë¶„ì„.jsonl',
                'line_num': item.get('line_num', 0),
                'location': location,
                'industry': industry,
                'relevance': relevance,
                'original_data': item
            })
    
    # ê´€ë ¨ì„± ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ Nê°œ ë°˜í™˜
    results.sort(key=lambda x: x['relevance'], reverse=True)
    return results[:top]

def search_biz_csv(query, location="", industry="", top=9):
    """ì—…ì¢…/ì§€ì—­ CSV ë°ì´í„° ê²€ìƒ‰"""
    results = []
    csv_files = []
    
    # ì—…ì¢…ë³„ CSV íŒŒì¼ë“¤ ì°¾ê¸°
    biz_dir = os.path.join(app.root_path, 'documents', 'raw', 'ì—…ì¢…')
    region_dir = os.path.join(app.root_path, 'documents', 'raw', 'ì§€ì—­')
    
    for directory in [biz_dir, region_dir]:
        if os.path.exists(directory):
            for filename in os.listdir(directory):
                if filename.endswith('.csv'):
                    csv_files.append(os.path.join(directory, filename))
    
    for csv_file in csv_files:
        try:
            df = pd.read_csv(csv_file, encoding='utf-8')
            for row_num, row in df.iterrows():
                # CSV í–‰ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                content = ' '.join([str(val) for val in row.values if pd.notna(val)])
                
                relevance = 0
                if query.lower() in content.lower():
                    relevance += 2
                if location and location.lower() in content.lower():
                    relevance += 1
                if industry and industry.lower() in content.lower():
                    relevance += 1
                
                if relevance > 0:
                    results.append({
                        'content': content,
                        'source_type': 'bizcsv',
                        'source_file': os.path.basename(csv_file),
                        'line_num': row_num + 1,
                        'location': location,
                        'industry': industry,
                        'relevance': relevance,
                        'original_data': row.to_dict()
                    })
        except Exception as e:
            print(f"Error reading {csv_file}: {e}")
            continue
    
    results.sort(key=lambda x: x['relevance'], reverse=True)
    return results[:top]

def search_did(query, location="", industry="", top=3):
    """DiD ë¶„ì„ ë°ì´í„° ê²€ìƒ‰"""
    results = []
    did_file = os.path.join(app.root_path, 'documents', 'raw', 'did.csv')
    
    if os.path.exists(did_file):
        try:
            df = pd.read_csv(did_file, encoding='utf-8')
            for row_num, row in df.iterrows():
                content = ' '.join([str(val) for val in row.values if pd.notna(val)])
                
                relevance = 0
                if query.lower() in content.lower():
                    relevance += 2
                if location and location.lower() in content.lower():
                    relevance += 1
                if industry and industry.lower() in content.lower():
                    relevance += 1
                
                if relevance > 0:
                    results.append({
                        'content': content,
                        'source_type': 'did',
                        'source_file': 'did.csv',
                        'line_num': row_num + 1,
                        'location': location,
                        'industry': industry,
                        'relevance': relevance,
                        'original_data': row.to_dict()
                    })
        except Exception as e:
            print(f"Error reading did.csv: {e}")
    
    results.sort(key=lambda x: x['relevance'], reverse=True)
    return results[:top]

def search_event_csv(query, location="", top=6, after=None):
    """ì´ë²¤íŠ¸ CSV ë°ì´í„° ê²€ìƒ‰"""
    results = []
    event_files = [
        'ì„±ë™êµ¬ ê³µí†µ_í•œì–‘ëŒ€_í¥í–‰ì˜í™” ì´ë²¤íŠ¸ DB.csv',
        'ì„±ìˆ˜ íŒì—… ìµœì¢….csv'
    ]
    
    for filename in event_files:
        file_path = os.path.join(app.root_path, 'documents', 'raw', filename)
        if os.path.exists(file_path):
            try:
                df = pd.read_csv(file_path, encoding='utf-8')
                for row_num, row in df.iterrows():
                    content = ' '.join([str(val) for val in row.values if pd.notna(val)])
                    
                    relevance = 0
                    if query.lower() in content.lower():
                        relevance += 2
                    if location and location.lower() in content.lower():
                        relevance += 1
                    
                    # ìµœì‹ ì„± ì²´í¬ (after ë‚ ì§œ ì´í›„ë§Œ)
                    if after and 'start_date' in row:
                        try:
                            from datetime import datetime
                            event_date = datetime.strptime(str(row['start_date']), '%Y.%m.%d')
                            if event_date < after:
                                continue
                        except:
                            pass
                    
                    if relevance > 0:
                        results.append({
                            'content': content,
                            'source_type': 'event',
                            'source_file': filename,
                            'line_num': row_num + 1,
                            'location': location,
                            'industry': '',
                            'relevance': relevance,
                            'original_data': row.to_dict()
                        })
            except Exception as e:
                print(f"Error reading {filename}: {e}")
                continue
    
    results.sort(key=lambda x: x['relevance'], reverse=True)
    return results[:top]

def retrieve_with_policy(task, query, location="", industry="", today=None):
    """RAG ì •ì±…ì— ë”°ë¥¸ ë¬¸ì„œ ê²€ìƒ‰ ë° ìŠ¤ì½”ì–´ë§"""
    policy = RAG_POLICY.get(task, RAG_POLICY["default"])
    buckets = {"card": [], "bizcsv": [], "did": [], "event": []}
    
    # 1) ì†ŒìŠ¤ë³„ 1ì°¨ í›„ë³´ ê²€ìƒ‰
    buckets["card"] = search_card_jsonl(query, location, industry, top=policy["quota"]["card"] * 3)
    buckets["bizcsv"] = search_biz_csv(query, location, industry, top=policy["quota"]["bizcsv"] * 3)
    buckets["did"] = search_did(query, location, industry, top=policy["quota"]["did"] * 3)
    
    # ìµœì‹ ì„± í•„í„°ë§ì„ ìœ„í•œ ë‚ ì§œ ê³„ì‚°
    if today is None:
        from datetime import datetime, timedelta
        today = datetime.now()
        after_date = today - timedelta(days=RECENCY_BUFFER_DAYS)
    else:
        after_date = today - timedelta(days=RECENCY_BUFFER_DAYS)
    
    buckets["event"] = search_event_csv(query, location, top=policy["quota"]["event"] * 3, after=after_date)
    
    # 2) ìŠ¤ì½”ì–´ë§ + ì •ë ¬
    def rank_items(items, source_type):
        return sorted(items, key=lambda x: calculate_final_score(x, source_type, policy["w"]), reverse=True)
    
    for source_type in buckets:
        buckets[source_type] = rank_items(buckets[source_type], source_type)
    
    # 3) ì¿¼í„° ì»· + ì¤‘ë³µ ì œê±° + í´ë°±
    picked = []
    picked_texts = []
    
    for source_type in ["card", "bizcsv", "did", "event"]:
        quota = policy["quota"][source_type]
        source_items = buckets[source_type]
        
        for item in source_items[:quota]:
            # ì¤‘ë³µ ì œê±° ì²´í¬
            is_duplicate = False
            for picked_text in picked_texts:
                if jaccard_similarity(item['content'], picked_text) > DUPLICATE_THRESHOLD:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                picked.append(item)
                picked_texts.append(item['content'])
    
    # 4) ê°•ì œ í¬í•¨ ê·œì¹™ ì²´í¬
    picked = ensure_mandatory_sources(picked, task, buckets, policy)
    
    # 5) ìŠ¤ë‹ˆí« ê¸¸ì´ ì¡°ì • + ì¶œì²˜ ì£¼ì„ ìƒì„±
    snippets = []
    for item in picked[:K_ANSWER]:
        content = truncate_with_sentence(item['content'], SNIPPET_MAX_LENGTH)
        citation = format_citation(item['source_type'], item['source_file'], item['line_num'])
        
        snippets.append({
            'content': content,
            'source_type': item['source_type'],
            'source_file': item['source_file'],
            'line_num': item['line_num'],
            'citation': citation,
            'expert_role': EXPERT_ROLES.get(task, EXPERT_ROLES["default"])
        })
    
    return snippets

def ensure_mandatory_sources(picked, task, buckets, policy):
    """í•„ìˆ˜ ì†ŒìŠ¤ í¬í•¨ ê·œì¹™ ì ìš©"""
    # ì‹ í•œì¹´ë“œ ìµœì†Œ 2ê°œ ê°•ì œ
    card_count = len([p for p in picked if p['source_type'] == 'card'])
    if card_count < 2 and buckets['card']:
        for item in buckets['card']:
            if item not in picked:
                picked.append(item)
                card_count += 1
                if card_count >= 2:
                    break
    
    # íƒœìŠ¤í¬ë³„ í•„ìˆ˜ ì†ŒìŠ¤ ì²´í¬
    if task == "trend" and len([p for p in picked if p['source_type'] == 'event']) == 0:
        if buckets['event']:
            picked.append(buckets['event'][0])
    
    if task in ["retention", "diagnosis"] and len([p for p in picked if p['source_type'] == 'did']) == 0:
        if buckets['did']:
            picked.append(buckets['did'][0])
    
    return picked

def detect_task_type(query):
    """ì§ˆë¬¸ ìœ í˜• ê°ì§€í•˜ì—¬ íƒœìŠ¤í¬ ë¼ìš°íŒ…"""
    query_lower = query.lower()
    
    # íŠ¸ë Œë“œ ê´€ë ¨ í‚¤ì›Œë“œ
    trend_keywords = ['íŠ¸ë Œë“œ', 'ì¸ê¸°', 'ìœ í–‰', 'ì‹œì¦Œ', 'ê³„ì ˆ', 'ë²šê½ƒ', 'ìº˜ë¦°ë”', 'ì´ë²¤íŠ¸', 'í–‰ì‚¬', 'ì‹œì¦Œ']
    if any(keyword in query_lower for keyword in trend_keywords):
        return "trend"
    
    # ê³ ê° ìœ ì§€ ê´€ë ¨ í‚¤ì›Œë“œ
    retention_keywords = ['ì¬ë°©ë¬¸', 'ê³ ê°ìœ ì§€', 'ë¦¬í…ì…˜', 'ë‹¨ê³¨', 'ì¶©ì„±', 'ë§Œì¡±ë„']
    if any(keyword in query_lower for keyword in retention_keywords):
        return "retention"
    
    # ë¬¸ì œ ì§„ë‹¨ ê´€ë ¨ í‚¤ì›Œë“œ
    diagnosis_keywords = ['ë¬¸ì œ', 'ì§„ë‹¨', 'ë¶„ì„', 'ì›ì¸', 'ì´ìœ ', 'ì™œ', 'ì–´ë–»ê²Œ', 'í•´ê²°']
    if any(keyword in query_lower for keyword in diagnosis_keywords):
        return "diagnosis"
    
    # ê³ ê° ì¶©ì„±ë„ ê´€ë ¨ í‚¤ì›Œë“œ
    loyalty_keywords = ['ì¶©ì„±ë„', 'ë¸Œëœë“œ', 'ì• ì°©', 'ì„ í˜¸ë„', 'ë§Œì¡±']
    if any(keyword in query_lower for keyword in loyalty_keywords):
        return "loyalty"
    
    # ì±„ë„ ê´€ë ¨ í‚¤ì›Œë“œ
    channel_keywords = ['ì±„ë„', 'ë§ˆì¼€íŒ…', 'ê´‘ê³ ', 'í™ë³´', 'ì†Œì…œë¯¸ë””ì–´', 'ì˜¨ë¼ì¸', 'ì˜¤í”„ë¼ì¸']
    if any(keyword in query_lower for keyword in channel_keywords):
        return "channel"
    
    # ê¸°ë³¸ê°’
    return "default"

def search_relevant_documents(query, location="", industry=""):
    """ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ í•¨ìˆ˜"""
    return retrieve_with_policy("default", query, location, industry)

def call_gemini_with_retry(model, prompt: str):
    """ì•ˆì •í™”ëœ Gemini API í˜¸ì¶œ"""
    generation_config = {
        "max_output_tokens": GEN_MAX_OUTPUT_TOKENS,
        "temperature": 0.7,
        "top_p": 0.9,
    }
    last_err = None

    for attempt in range(RETRY_MAX):
        try:
            print(f"ğŸ”„ Gemini API í˜¸ì¶œ ì‹œë„ {attempt+1}/{RETRY_MAX}")
            print(f"ğŸ“ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}ì")
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as ex:
                fut = ex.submit(
                    model.generate_content,
                    prompt,
                    generation_config=generation_config,
                )
                resp = fut.result(timeout=GEN_TIMEOUT_SEC)

            print(f"âœ… Gemini API ì‘ë‹µ ë°›ìŒ: type={type(resp)}")
            
            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            try:
                text_output = resp.text
            except Exception as e:
                print(f"âš ï¸ ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                text_output = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë‹¤ì‹œ ê°„ë‹¨íˆ í•´ì£¼ì‹œê±°ë‚˜ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ë¬¸ì˜í•´ ì£¼ì„¸ìš”."
            
            if not text_output or len(text_output.strip()) < 10:
                text_output = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë‹¤ì‹œ ê°„ë‹¨íˆ í•´ì£¼ì‹œê±°ë‚˜ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ë¬¸ì˜í•´ ì£¼ì„¸ìš”."
                print("âš ï¸ ë¹ˆ ì‘ë‹µìœ¼ë¡œ ì¸í•œ ì•ˆì „ ë¬¸êµ¬ ì‚¬ìš©")
            
            print(f"ğŸ“¤ ìµœì¢… ì‘ë‹µ ê¸¸ì´: {len(text_output)}ì")
            return text_output

        except concurrent.futures.TimeoutError:
            last_err = TimeoutError(f"generate_content timeout ({GEN_TIMEOUT_SEC}s)")
        except Exception as e:
            last_err = e
            msg = str(e)
            if is_overloaded_error(msg) and attempt < RETRY_MAX - 1:
                backoff = (RETRY_BASE ** attempt) + random.uniform(0, RETRY_JITTER)
                print(f"âš ï¸ ì¼ì‹œ ì˜¤ë¥˜(ì¬ì‹œë„ {attempt+1}/{RETRY_MAX}): {msg} â†’ {backoff:.2f}s ëŒ€ê¸°")
                time.sleep(backoff)
            else:
                print(f"âŒ API Error: {msg}")
                break

    # ìµœì¢… ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë©”ì‹œì§€ ë°˜í™˜
    error_msg = f"API í˜¸ì¶œ ì‹¤íŒ¨: {str(last_err)}"
    print(f"âŒ {error_msg}")
    return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ({error_msg})"

# ====== ë¼ìš°íŠ¸ ======
@app.route('/')
def index():
    """ë©”ì¸ í˜ì´ì§€"""
    return render_template('index.html')

@app.route('/setup')
def setup():
    """ì„¤ì • í˜ì´ì§€"""
    return render_template('setup.html')

@app.route('/chat')
def chat():
    """ì±„íŒ… í˜ì´ì§€"""
    return render_template('chat.html')

@app.route('/api/check-setup', methods=['GET'])
def check_setup():
    """ì„¤ì • í™•ì¸"""
    setup_exists = 'default' in user_setups and user_setups['default']
    setup_info = user_setups.get('default', {}) if setup_exists else None
    
    return jsonify({
        'setup_exists': setup_exists,
        'is_setup': setup_exists,
        'setup_info': setup_info
    })

@app.route('/api/setup', methods=['POST'])
def save_setup():
    """ì‚¬ìš©ì ì„¤ì • ì €ì¥"""
    try:
        data = request.get_json()
        location = data.get('location', '')
        industry = data.get('industry', '')
        store_name = data.get('store_name', '')
        
        user_setups['default'] = {
            'location': location,
            'industry': industry,
            'store_name': store_name
        }
        
        print(f"ğŸ“ ì§€ì—­: {location}, ğŸ¢ ì—…ì¢…: {industry}, ğŸª ê°€ê²Œ: {store_name}")
        
        return jsonify({'success': True, 'message': 'ì„¤ì •ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.'})
    
    except Exception as e:
        print(f"Error saving setup: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/get-setup', methods=['GET'])
def get_setup():
    """ì‚¬ìš©ì ì„¤ì • ì¡°íšŒ"""
    setup = user_setups.get('default', {})
    return jsonify(setup)

@app.route('/api/calendar-events', methods=['GET'])
def get_calendar_events():
    """ë‹¬ë ¥ ì´ë²¤íŠ¸ ì¡°íšŒ"""
    try:
        events = load_calendar_events()
        return jsonify(events)
    except Exception as e:
        print(f"Error loading calendar events: {e}")
        return jsonify([])

@app.route('/api/chat', methods=['POST'])
def chat_api():
    """ì±„íŒ… API"""
    try:
        data = request.get_json()
        user_message = data.get('message', '')
        session_id = data.get('session_id', 'default')
        
        if not user_message:
            return jsonify({'error': 'ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.'}), 400
        
        # ì‚¬ìš©ì ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        setup = user_setups.get('default', {})
        location = setup.get('location', '')
        industry = setup.get('industry', '')
        store_name = setup.get('store_name', '')
        
        # ìºì‹œ í™•ì¸
        cache_key = f"{user_message}_{location}_{industry}"
        if cache_key in response_cache:
            print("ğŸ’¾ ìºì‹œì—ì„œ ì‘ë‹µ ë°˜í™˜")
            return jsonify({
                'message': response_cache[cache_key],
                'session_id': session_id
            })
        
        # Gemini API í˜¸ì¶œ
        model = get_model()
        if model is None:
            return jsonify({
                'message': 'âŒ Google API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.',
                'status': 'error'
            }), 500
        
        # íƒœìŠ¤í¬ ë¼ìš°íŒ… (ì§ˆë¬¸ ìœ í˜• ê°ì§€)
        task = detect_task_type(user_message)
        
        # ìƒˆë¡œìš´ RAG ì‹œìŠ¤í…œìœ¼ë¡œ ë¬¸ì„œ ê²€ìƒ‰
        relevant_snippets = retrieve_with_policy(task, user_message, location, industry)
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        context_info = ""
        if location or industry or store_name:
            context_info = f"""
ğŸ“ ì„ íƒëœ ì§€ì—­: {location if location else 'ë¯¸ì„ íƒ'}
ğŸª ì„ íƒëœ ì—…ì¢…: {industry if industry else 'ë¯¸ì„ íƒ'}
ğŸ¢ ê°€ê²Œëª…: {store_name if store_name else 'ë¯¸ì„ íƒ'}

ìœ„ì˜ ì§€ì—­ê³¼ ì—…ì¢… ì •ë³´ë¥¼ ê³ ë ¤í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
        
        # ì „ë¬¸ê°€ ì—­í•  ì‹ë³„
        expert_role = EXPERT_ROLES.get(task, EXPERT_ROLES["default"])
        
        # RAG ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ (ì¶œì²˜ í¬í•¨)
        rag_context = ""
        if relevant_snippets:
            rag_context = "\n\n[ì°¸ê³  ë°ì´í„° - ì¶œì²˜ í¬í•¨]\n"
            for i, snippet in enumerate(relevant_snippets, 1):
                rag_context += f"{i}. {snippet['content']} {snippet['citation']}\n"
        
        prompt = f"""
ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” {expert_role}ë¡œì„œ ì„±ë™êµ¬ ì†Œìƒê³µì¸ ì—¬ëŸ¬ë¶„ì„ ë„ì™€ë“œë¦½ë‹ˆë‹¤.

{context_info}
{rag_context}

ì§ˆë¬¸: {user_message}

ìœ„ì˜ ì§€ì—­ê³¼ ì—…ì¢… ì •ë³´ë¥¼ ë°˜ë“œì‹œ ê³ ë ¤í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.
ì°¸ê³  ë°ì´í„°ê°€ ìˆë‹¤ë©´ ì¶œì²˜ì™€ í•¨ê»˜ ì ê·¹ì ìœ¼ë¡œ í™œìš©í•´ì£¼ì„¸ìš”.
ëª¨ë“  ìˆ˜ì¹˜ë‚˜ ì£¼ì¥ì—ëŠ” ë°˜ë“œì‹œ ì¶œì²˜ë¥¼ í‘œê¸°í•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ ì‹¤ìš©ì ì´ê³  êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
        
        # Gemini API í˜¸ì¶œ
        start_time = time.time()
        response_text = call_gemini_with_retry(model, prompt)
        
        # ë§ˆí¬ë‹¤ìš´ ë³€í™˜
        html_response = safe_markdown(response_text)
        
        # ìºì‹œ ì €ì¥
        response_cache[cache_key] = html_response
        
        # ì²˜ë¦¬ ì‹œê°„ ë¡œê¹…
        processing_time = time.time() - start_time
        print(f"â±ï¸ API ì‘ë‹µ ì‹œê°„: {processing_time:.2f}ì´ˆ")
        
        return jsonify({
            'message': html_response,
            'session_id': session_id
        })
    
    except Exception as e:
        print(f"Error in chat API: {e}")
        return jsonify({'error': f'ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'}), 500

@app.route('/api/reset', methods=['POST'])
def reset_chat():
    """ì±„íŒ… ë¦¬ì…‹"""
    global response_cache
    response_cache.clear()
    return jsonify({'success': True, 'message': 'ì±„íŒ…ì´ ë¦¬ì…‹ë˜ì—ˆìŠµë‹ˆë‹¤.'})

@app.route('/health', methods=['GET'])
def health_check():
    """í—¬ìŠ¤ì²´í¬"""
    return jsonify({'status': 'ok'}), 200

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 8080))
    print(f"ğŸš€ Flask ì„œë²„ ì‹œì‘: 0.0.0.0:{port}")
    app.run(debug=False, host='0.0.0.0', port=port)
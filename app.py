from flask import Flask, request, jsonify, render_template, redirect
from flask_cors import CORS
import google.generativeai as genai
import os
from dotenv import load_dotenv
import markdown
import pandas as pd
from datetime import datetime, timedelta
import glob
import re
from pathlib import Path
import csv
import json

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

app = Flask(__name__)
CORS(app)

# Gemini API ì„¤ì • (ì§€ì—° ë¡œë”©)
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
_model = None  # ì§€ì—° ë¡œë”©ì„ ìœ„í•œ ì „ì—­ ë³€ìˆ˜

def get_model():
    """ì§€ì—° ë¡œë”©ìœ¼ë¡œ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°"""
    global _model
    if _model is None:
        if not GOOGLE_API_KEY:
            print("âš ï¸ ê²½ê³ : GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("Render Dashboard â†’ Environmentì—ì„œ GOOGLE_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
            return None
        else:
            print("âœ… Google API Keyê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
            genai.configure(api_key=GOOGLE_API_KEY)
            # Gemini Flash 2.5 ëª¨ë¸ ì„¤ì • (ë¬´ë£Œ ë²„ì „)
            _model = genai.GenerativeModel('gemini-2.5-flash')
            print("ğŸ¤– Gemini ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    return _model

# ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥
chat_sessions = {}

# RAG ë¬¸ì„œ ì €ì¥ì†Œ (ì§€ì—° ë¡œë”©)
_rag_documents = None
_document_index = None
_response_cache = {}

# ì „ì—­ ë³€ìˆ˜ë“¤
rag_documents = {}
document_index = {}
response_cache = {}

def get_rag_documents():
    """ì§€ì—° ë¡œë”©ìœ¼ë¡œ RAG ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°"""
    global _rag_documents, rag_documents
    if _rag_documents is None:
        print("ğŸ“š RAG ë¬¸ì„œ ë¡œë”© ì‹œì‘...")
        _rag_documents = load_rag_documents()
        rag_documents = _rag_documents  # ì „ì—­ ë³€ìˆ˜ë„ ì—…ë°ì´íŠ¸
        print(f"ğŸ“š ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {len(_rag_documents)}ê°œ")
    return _rag_documents

def get_document_index():
    """ì§€ì—° ë¡œë”©ìœ¼ë¡œ ë¬¸ì„œ ì¸ë±ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    global _document_index
    if _document_index is None:
        print("ğŸ” ë¬¸ì„œ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹œì‘...")
        _document_index = build_document_index()
        print(f"ğŸ” ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {len(_document_index['keywords'])}ê°œ í‚¤ì›Œë“œ")
    return _document_index

def load_rag_documents():
    """documents/raw í´ë”ì˜ ëª¨ë“  íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ RAG ì‹œìŠ¤í…œì— ì €ì¥"""
    global rag_documents
    rag_documents = {}
    
    # documents/raw í´ë”ì˜ ëª¨ë“  íŒŒì¼ ì°¾ê¸°
    raw_folder = Path('documents/raw')
    if not raw_folder.exists():
        return rag_documents
    
    # ì§€ì›í•˜ëŠ” íŒŒì¼ í˜•ì‹
    supported_extensions = ['.txt', '.md', '.csv', '.json', '.jsonl', '.ipynb']
    
    for file_path in raw_folder.glob('*'):
        if file_path.suffix.lower() in supported_extensions:
            try:
                if file_path.suffix.lower() == '.csv':
                    # CSV íŒŒì¼ ì²˜ë¦¬
                    df = pd.read_csv(file_path, encoding='utf-8')
                    content = f"íŒŒì¼ëª…: {file_path.name}\n\n"
                    content += f"ë°ì´í„° í˜•íƒœ: CSV\n"
                    content += f"í–‰ ìˆ˜: {len(df)}\n"
                    content += f"ì—´: {', '.join(df.columns.tolist())}\n\n"
                    
                    # ì²˜ìŒ 5í–‰ì˜ ë°ì´í„° ìƒ˜í”Œ
                    content += "ë°ì´í„° ìƒ˜í”Œ:\n"
                    content += df.head().to_string()
                    
                elif file_path.suffix.lower() == '.md':
                    # Markdown íŒŒì¼ ì²˜ë¦¬
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                elif file_path.suffix.lower() == '.txt':
                    # í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        
                elif file_path.suffix.lower() == '.json':
                    # JSON íŒŒì¼ ì²˜ë¦¬
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        
                elif file_path.suffix.lower() == '.jsonl':
                    # JSONL íŒŒì¼ ì²˜ë¦¬
                    import json
                    content = f"íŒŒì¼ëª…: {file_path.name}\n\n"
                    content += f"ë°ì´í„° í˜•íƒœ: JSONL (JSON Lines)\n\n"
                    
                    with open(file_path, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                    
                    content += f"ì´ ë¼ì¸ ìˆ˜: {len(lines)}\n\n"
                    content += "ë°ì´í„° ë‚´ìš©:\n"
                    
                    # ì‹ í•œì¹´ë“œë¶„ì„.jsonlì€ ì „ì²´ ë ˆì½”ë“œ ì²˜ë¦¬ (ì¤‘ìš”í•œ ë°ì´í„°)
                    if 'ì‹ í•œì¹´ë“œë¶„ì„.jsonl' in file_path.name:
                        max_records = len(lines)  # ì „ì²´ ë ˆì½”ë“œ ì²˜ë¦¬
                    else:
                        max_records = 5  # ë‹¤ë¥¸ íŒŒì¼ì€ ë©”ëª¨ë¦¬ ìµœì í™”
                    for i, line in enumerate(lines[:max_records]):
                        if line.strip():  # ë¹ˆ ì¤„ì´ ì•„ë‹Œ ê²½ìš°ë§Œ
                            try:
                                data = json.loads(line.strip())
                                content += f"--- ë ˆì½”ë“œ {i+1} ---\n"
                                for key, value in data.items():
                                    content += f"{key}: {value}\n"
                                content += "\n"
                            except json.JSONDecodeError:
                                content += f"--- ë ˆì½”ë“œ {i+1} (JSON íŒŒì‹± ì˜¤ë¥˜) ---\n{line.strip()}\n\n"
                    
                    if len(lines) > max_records:
                        content += f"... (ì´ {len(lines)}ê°œ ë ˆì½”ë“œ ì¤‘ {max_records}ê°œë§Œ í‘œì‹œ)\n"
                        
                elif file_path.suffix.lower() == '.ipynb':
                    # Jupyter Notebook íŒŒì¼ ì²˜ë¦¬
                    import json
                    with open(file_path, 'r', encoding='utf-8') as f:
                        notebook = json.load(f)
                    
                    content = f"íŒŒì¼ëª…: {file_path.name}\n\n"
                    content += f"ë…¸íŠ¸ë¶ íƒ€ì…: Jupyter Notebook\n"
                    content += f"ì…€ ìˆ˜: {len(notebook.get('cells', []))}\n\n"
                    
                    # ê° ì…€ì˜ ë‚´ìš© ì¶”ì¶œ
                    for i, cell in enumerate(notebook.get('cells', [])):
                        cell_type = cell.get('cell_type', 'unknown')
                        source = ''.join(cell.get('source', []))
                        
                        if cell_type == 'markdown':
                            content += f"## ì…€ {i+1} (ë§ˆí¬ë‹¤ìš´):\n{source}\n\n"
                        elif cell_type == 'code':
                            content += f"## ì…€ {i+1} (ì½”ë“œ):\n```python\n{source}\n```\n\n"
                        elif cell_type == 'raw':
                            content += f"## ì…€ {i+1} (í…ìŠ¤íŠ¸):\n{source}\n\n"
                
                # ë¬¸ì„œ ì €ì¥
                rag_documents[file_path.name] = {
                    'content': content,
                    'file_type': file_path.suffix.lower(),
                    'file_path': str(file_path)
                }
                
            except Exception as e:
                print(f"íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ {file_path.name}: {e}")
                continue
    
    return rag_documents

def build_document_index():
    """ë¬¸ì„œ ì¸ë±ìŠ¤ êµ¬ì¶• (í‚¤ì›Œë“œ, ì¹´í…Œê³ ë¦¬, ê°œì²´ëª… ì¶”ì¶œ)"""
    global document_index
    document_index = {'keywords': {}, 'categories': {}, 'entities': {}}
    
    # í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë§¤í•‘
    for filename, doc_info in rag_documents.items():
        content = doc_info['content'].lower()
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ í˜•íƒœì†Œ ë¶„ì„)
        keywords = extract_keywords(content)
        for keyword in keywords:
            if keyword not in document_index['keywords']:
                document_index['keywords'][keyword] = []
            document_index['keywords'][keyword].append(filename)
        
        # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        category = classify_document(filename, content)
        if category not in document_index['categories']:
            document_index['categories'][category] = []
        document_index['categories'][category].append(filename)
    
    return document_index

def extract_keywords(text):
    """í…ìŠ¤íŠ¸ì—ì„œ ì¤‘ìš”í•œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ NLP í•„ìš”)
    important_words = ['íŒì—…', 'ì´ë²¤íŠ¸', 'ì„±ë™êµ¬', 'ì„±ìˆ˜', 'ë§ˆì¼€íŒ…', 'ê³ ê°', 'ë§¤ì¶œ', 'ë¶„ì„', 'ë°ì´í„°']
    found_keywords = []
    for word in important_words:
        if word in text:
            found_keywords.append(word)
    return found_keywords

def classify_document(filename, content):
    """ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
    if 'ì‹ í•œì¹´ë“œ' in filename.lower():
        return 'ë°ì´í„°ë¶„ì„'
    elif 'íŒì—…' in filename.lower() or 'ì´ë²¤íŠ¸' in filename.lower():
        return 'ì´ë²¤íŠ¸'
    elif 'ë§ˆì¼€íŒ…' in content:
        return 'ë§ˆì¼€íŒ…'
    else:
        return 'ê¸°íƒ€'

def search_relevant_documents(query, max_docs=3):
    """ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰: ì¸ë±ìŠ¤ ê¸°ë°˜ ë¹ ë¥¸ ê²€ìƒ‰ (ì§€ì—° ë¡œë”©)"""
    # ì§€ì—° ë¡œë”©ìœ¼ë¡œ ë¬¸ì„œì™€ ì¸ë±ìŠ¤ ê°€ì ¸ì˜¤ê¸°
    rag_documents = get_rag_documents()
    document_index = get_document_index()
    
    if not rag_documents or rag_documents is None:
        return []
    
    query_lower = query.lower()
    relevant_docs = []
    
    # 1. ì‹ í•œì¹´ë“œë¶„ì„.jsonl íŒŒì¼ ìš°ì„  ì²˜ë¦¬
    shinhan_file = None
    for filename in rag_documents.keys():
        if 'ì‹ í•œì¹´ë“œë¶„ì„.jsonl' in filename:
            shinhan_file = filename
            break
    
    # ëª¨ë“  ì§ˆë¬¸ì— ëŒ€í•´ ì‹ í•œì¹´ë“œë¶„ì„.jsonlì„ ìµœìš°ì„ ìœ¼ë¡œ í¬í•¨
    if shinhan_file:
        doc_info = rag_documents[shinhan_file]
        relevant_docs.append({
            'filename': shinhan_file,
            'content': doc_info['content'][:8000],  # ì‹ í•œì¹´ë“œ íŒŒì¼ì€ ë” ë§ì€ ë¬¸ì ì‚¬ìš© (3000 â†’ 8000)
            'relevance_score': 1000  # ìµœê³  ìš°ì„ ìˆœìœ„ (ê¸°ì¡´ 100ì—ì„œ 1000ìœ¼ë¡œ ì¦ê°€)
        })
    
    # 2. í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ (ì¸ë±ìŠ¤ í™œìš©)
    candidate_files = set()
    for keyword in query_lower.split():
        if keyword in document_index['keywords']:
            candidate_files.update(document_index['keywords'][keyword])
    
    # 2. ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ê²€ìƒ‰
    if 'íŒì—…' in query_lower or 'ì´ë²¤íŠ¸' in query_lower:
        if 'ì´ë²¤íŠ¸' in document_index['categories']:
            candidate_files.update(document_index['categories']['ì´ë²¤íŠ¸'])
    
    if 'ë°ì´í„°' in query_lower or 'ë¶„ì„' in query_lower:
        if 'ë°ì´í„°ë¶„ì„' in document_index['categories']:
            candidate_files.update(document_index['categories']['ë°ì´í„°ë¶„ì„'])
    
    # 3. í›„ë³´ íŒŒì¼ë“¤ ì¤‘ì—ì„œ ê´€ë ¨ë„ ê³„ì‚°
    for filename in candidate_files:
        if filename in rag_documents:
            doc_info = rag_documents[filename]
            content = doc_info['content'].lower()
            
            # ê´€ë ¨ë„ ì ìˆ˜ ê³„ì‚°
            relevance_score = 0
            for keyword in query_lower.split():
                if keyword in content:
                    relevance_score += content.count(keyword)
                if keyword in filename.lower():
                    relevance_score += 2
            
            # ì‹ í•œì¹´ë“œë¶„ì„.jsonl íŒŒì¼ì— ì¶”ê°€ ê°€ì¤‘ì¹˜ (ìµœìš°ì„ )
            if 'ì‹ í•œì¹´ë“œë¶„ì„.jsonl' in filename:
                relevance_score += 500  # ê¸°ì¡´ 50ì—ì„œ 500ìœ¼ë¡œ ëŒ€í­ ì¦ê°€
            
            # ì‹ í•œì¹´ë“œ ë°ì´í„°ëŠ” ë” ë§ì€ ë¬¸ì ì‚¬ìš© (ì œí•œ ê°•í™”)
            max_chars = 8000 if 'ì‹ í•œì¹´ë“œ' in filename.lower() or 'shinhan' in filename.lower() else 500
            
            relevant_docs.append({
                'filename': filename,
                'content': doc_info['content'][:max_chars],
                'relevance_score': relevance_score
            })
    
    # ê´€ë ¨ë„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ ë¬¸ì„œë§Œ ë°˜í™˜
    relevant_docs.sort(key=lambda x: x['relevance_score'], reverse=True)
    return relevant_docs[:max_docs]

# Render í™˜ê²½ì—ì„œ ì•± ì‹œì‘ ì‹œ ë¬¸ì„œ ë¡œë”© ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)
print("ğŸš€ Render í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘ - ë¬¸ì„œ ë¡œë”©ì€ ìš”ì²­ ì‹œ ì²˜ë¦¬ë©ë‹ˆë‹¤.")
rag_documents = {}
document_index = {'keywords': {}, 'categories': {}, 'entities': {}}

@app.route('/api/reload-documents', methods=['POST'])
def reload_documents():
    """ë¬¸ì„œë¥¼ ë‹¤ì‹œ ë¡œë“œí•˜ëŠ” API"""
    try:
        load_rag_documents()
        return jsonify({
            'success': True, 
            'message': f'{len(rag_documents)}ê°œì˜ ë¬¸ì„œê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.',
            'documents': list(rag_documents.keys())
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/documents', methods=['GET'])
def get_documents():
    """í˜„ì¬ ë¡œë“œëœ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ"""
    return jsonify({
        'documents': [
            {
                'filename': filename,
                'file_type': doc_info['file_type'],
                'content_length': len(doc_info['content'])
            }
            for filename, doc_info in rag_documents.items()
        ]
    })

# ì´ë²¤íŠ¸ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
def load_event_data():
    events = {}
    
    try:
        # ì„±ë™êµ¬ ê³µí†µ ì´ë²¤íŠ¸ ë°ì´í„° ë¡œë“œ
        common_events_df = pd.read_csv('documents/raw/ì„±ë™êµ¬ ê³µí†µ_í•œì–‘ëŒ€_í¥í–‰ì˜í™” ì´ë²¤íŠ¸ DB.csv', encoding='utf-8')
        
        for _, row in common_events_df.iterrows():
            start_date = pd.to_datetime(row['Start_Date'], format='%Y.%m.%d', errors='coerce')
            end_date = pd.to_datetime(row['End_Date'], format='%Y.%m.%d', errors='coerce')
            
            if pd.isna(start_date) or pd.isna(end_date):
                continue
                
            # ì´ë²¤íŠ¸ íƒ€ì… ë¶„ë¥˜
            event_type = 'general'
            if 'íŒì—…' in str(row['Event_Type']):
                event_type = 'popup'
            elif 'ëŒ€í•™' in str(row['Event_Type']) or 'í•œì–‘ëŒ€' in str(row['Event_Name']):
                event_type = 'university'
            elif 'ì˜í™”' in str(row['Event_Type']) or 'ê°œë´‰' in str(row['Event_Name']):
                event_type = 'movie'
            
            # ë‚ ì§œ ë²”ìœ„ì— ë”°ë¼ ì´ë²¤íŠ¸ ì¶”ê°€
            current_date = start_date
            while current_date <= end_date:
                date_str = current_date.strftime('%Y-%m-%d')
                if date_str not in events:
                    events[date_str] = []
                
                events[date_str].append({
                    'name': row['Event_Name'],
                    'type': event_type,
                    'startDate': start_date.strftime('%Y-%m-%d'),
                    'endDate': end_date.strftime('%Y-%m-%d'),
                    'location': row['Location_Address'],
                    'target': row['Target_Audience'],
                    'description': row['Event_Description']
                })
                
                current_date += timedelta(days=1)
                
    except Exception as e:
        print(f"ê³µí†µ ì´ë²¤íŠ¸ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    try:
        # ì„±ìˆ˜ íŒì—… ë°ì´í„° ë¡œë“œ
        popup_events_df = pd.read_csv('documents/raw/ì„±ìˆ˜ íŒì—… ìµœì¢….csv', encoding='utf-8')
        
        for _, row in popup_events_df.iterrows():
            start_date = pd.to_datetime(row['Start_Date'], format='%Y.%m.%d', errors='coerce')
            end_date = pd.to_datetime(row['End_Date'], format='%Y.%m.%d', errors='coerce')
            
            if pd.isna(start_date) or pd.isna(end_date):
                continue
            
            # ë‚ ì§œ ë²”ìœ„ì— ë”°ë¼ ì´ë²¤íŠ¸ ì¶”ê°€
            current_date = start_date
            while current_date <= end_date:
                date_str = current_date.strftime('%Y-%m-%d')
                if date_str not in events:
                    events[date_str] = []
                
                events[date_str].append({
                    'name': row['Event_Name'],
                    'type': 'popup',
                    'startDate': start_date.strftime('%Y-%m-%d'),
                    'endDate': end_date.strftime('%Y-%m-%d'),
                    'location': row['Location_Address'],
                    'target': row['Target_Audience'],
                    'description': row['Event_Description']
                })
                
                current_date += timedelta(days=1)
                
    except Exception as e:
        print(f"íŒì—… ì´ë²¤íŠ¸ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return events

@app.route('/')
def index():
    # ë©”ì¸ í˜ì´ì§€ëŠ” ì„¤ì • í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
    return redirect('/setup')

@app.route('/setup')
def setup():
    return render_template('setup.html')

@app.route('/chat')
def chat_page():
    return render_template('chat.html')

@app.route('/calendar')
def calendar():
    return render_template('calendar.html')

@app.route('/api/chat', methods=['POST'])
def chat():
    try:
        data = request.json
        user_message = data.get('message', '')
        session_id = data.get('session_id', 'default')
        
        if not user_message:
            return jsonify({'error': 'ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.'}), 400
        
        # ì €ì¥ëœ ì„¤ì • ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        setup_info = user_setups.get('default', None)
        if not setup_info:
            return jsonify({'error': 'ì„¤ì •ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ì„¤ì •ì„ ì™„ë£Œí•´ì£¼ì„¸ìš”.'}), 400
        
        location = setup_info['location']
        industry = setup_info['industry']
        store_name = setup_info['store_name']
        
        print(f"ğŸ“ ì§€ì—­: {location}, ğŸ¢ ì—…ì¢…: {industry}, ğŸª ê°€ê²Œ: {store_name}")
        
        # ì„¸ì…˜ë³„ ì±„íŒ… íˆìŠ¤í† ë¦¬ ê´€ë¦¬
        if session_id not in chat_sessions:
            model = get_model()
            if model is None:
                return jsonify({
                    'message': 'âŒ Google API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.',
                    'status': 'error'
                }), 500
            chat_sessions[session_id] = model.start_chat(history=[])
        
        chat = chat_sessions[session_id]
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„± (ìƒˆë¡œìš´ í…œí”Œë¦¿)
        system_prompt = f"""[SYSTEM / ROLE]
ë„ˆëŠ” ì„±ë™êµ¬ ì†Œìƒê³µì¸ì„ ìœ„í•œ ë§ì¶¤ ë§ˆì¼€íŒ… ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ, ì‹¤í–‰ ê°€ëŠ¥í•œ TODOë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ì œì‹œí•œë‹¤.

[CONTEXT PRIORITY]
- 1ìˆœìœ„: ì‹ í•œì¹´ë“œë¶„ì„.jsonl (í‚¤: INS, RULE, POPUP, ìƒê¶ŒíŠ¹ì„± ë“±)  
- 2ìˆœìœ„: ì§€ì—­/ì›”/ì—…ì¢…ê³¼ ì§ì ‘ ì—°ê´€ëœ CSV/JSON (ì˜ˆ: "ì„±ìˆ˜ íŒì—… ìµœì¢….csv", "ì„±ë™êµ¬ ê³µí†µ_í•œì–‘ëŒ€_í¥í–‰ì˜í™” ì´ë²¤íŠ¸ DB.csv", ê¸°íƒ€ ìƒê¶Œ/í–‰ì‚¬ DB)
- ê°™ì€ ì •ë³´ê°€ ì¤‘ë³µì¼ ë• 1ìˆœìœ„ë¥¼ ìš°ì„  ì±„íƒí•œë‹¤.

[STRICT CITATION]
- ëª¨ë“  ì£¼ì¥/ìˆ«ì/ì‚¬ì‹¤ ë’¤ì— (ì¶œì²˜: íŒŒì¼ëª…[#ë ˆì½”ë“œID]) í˜•ì‹ìœ¼ë¡œ ê·¼ê±°ë¥¼ í‘œê¸°í•œë‹¤.
  - ì˜ˆ: (ì¶œì²˜: ì‹ í•œì¹´ë“œë¶„ì„.jsonl#INS-12, RULE-3), (ì¶œì²˜: ì„±ìˆ˜ íŒì—… ìµœì¢….csv#row128)
- ì¶œì²˜ê°€ ë¶ˆëª…í™•í•˜ë©´ "ë°ì´í„° ë¯¸í™•ì¸"ì´ë¼ê³  ëª…ì‹œí•˜ê³  ì¶”ì • ë°œí™”ë¥¼ í•˜ì§€ ì•ŠëŠ”ë‹¤.

[RETRIEVAL SCOPE]
- REGION={location}, INDUSTRY={industry}, STORE={store_name}, MONTH=í˜„ì¬ì›”
- ë¨¼ì € REGIONÃ—INDUSTRY í‚¤ë¡œ íŒŒí‹°ì…˜ì„ ì¢í˜€ L0 í”„ë¡œí•„ ë¬¸ì„œë¥¼ ë¡œë“œí•œë‹¤.
- ë¶€ì¡±í•  ë•Œë§Œ ë™ì¼ íŒŒí‹°ì…˜ì—ì„œ Top-K=3, ìœˆë„ìš°=400~600ì ìŠ¤ë‹ˆí«ìœ¼ë¡œ L1 ìŠ¬ë¦¼ RAG ë³´ê°•í•œë‹¤.
- ê²€ìƒ‰ì–´ëŠ” `{location} {industry} í˜„ì¬ì›” íŒì—…/ì´ë²¤íŠ¸/ì†Œë¹„íŒ¨í„´/ì‹œê°„ëŒ€/íƒ€ê¹ƒ` ì¤‘ì‹¬ìœ¼ë¡œ í™•ì¥í•œë‹¤.
- ìµœì‹ ì„±ì´ í•„ìš”í•œ í•­ëª©(ì´ë‹¬ í–‰ì‚¬ ë“±)ì€ ìµœì‹  ì›” ìš°ì„  ì •ë ¬í•œë‹¤.

[STYLE & TONE]
- ì²« ë¬¸ì¥ ê³ ì •: "{location} ì§€ì—­ì˜ {industry} ì‚¬ì¥ë‹˜, ì•ˆë…•í•˜ì„¸ìš”. í˜„ì¬ì›” ë§ˆì¼€íŒ… ê°€ì´ë“œë¥¼ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤."
- ë¬¸ë‹¨ì€ ì§§ê²Œ, ë¦¬ìŠ¤íŠ¸/í‘œë¥¼ ì ê·¹ í™œìš©. ì‚¬ì¥ë‹˜ì´ ë°”ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆë„ë¡ ìˆ˜ì¹˜Â·í–‰ë™Â·íˆ´ì„ êµ¬ì²´í™”.
- ì§€ì—­ íŠ¹ì§•ê³¼ ì—…ì¢… íŠ¹ì„±ì„ "êµì§‘í•© ê´€ì "ìœ¼ë¡œ ì œì‹œ.

[OUTPUT FORMAT]
# â˜• {location} {industry}ë¥¼ ìœ„í•œ í˜„ì¬ì›” ë§ì¶¤í˜• ë§ˆì¼€íŒ… ì „ëµ

ê°„ë‹¨ìš”ì•½(2~3ë¬¸ì¥) â€” í•µì‹¬ ì¸ì‚¬ì´íŠ¸ì™€ ì´ë²ˆ ë‹¬ ê¸°íšŒ í¬ì¸íŠ¸. (ì¶œì²˜: â€¦)

1. ìƒê¶ŒÂ·ìˆ˜ìš” í•µì‹¬ í¬ì¸íŠ¸
- â— ìœ ë™/ì—°ë ¹/ì‹œê°„ëŒ€/ê°ë‹¨ê°€ í•µì‹¬ ê´€ì°° 3~5ê°œ (ìˆ«ì/ê·¼ê±° í¬í•¨). (ì¶œì²˜: â€¦)

2. ì´ë²ˆ ë‹¬( í˜„ì¬ì›” ) ì´ë²¤íŠ¸/íŒì—… ì—°ê³„ ì•„ì´ë””ì–´
- â— ì•„ì´ë””ì–´ëª… â€” ì™œ/ì–´ë–»ê²Œ/ì˜ˆìƒíš¨ê³¼/ê°„ë‹¨ ì‹¤í–‰ ì ˆì°¨. (ì¶œì²˜: â€¦)
- â— â€¦

3. ì±„ë„ë³„ ì‹¤ì „ ì•¡ì…˜(ì´ë²ˆ ì£¼ ë°”ë¡œ ì‹¤í–‰)
- [ ] ë„¤ì´ë²„í”Œë ˆì´ìŠ¤: í‚¤ì›Œë“œ/í•´ì‹œíƒœê·¸/ë¦¬ë·° ë¦¬í”„ë ˆì´ë°(ì˜ˆì‹œ ë¬¸êµ¬). (ì¶œì²˜: â€¦)
- [ ] ì¸ìŠ¤íƒ€ ë¦´ìŠ¤: ìº˜ë¦°ë” ì—°ë™/ì½˜í…ì¸  í…Œë§ˆ/ì—…ë¡œë“œ ì‹œê°. (ì¶œì²˜: â€¦)
- [ ] ì˜¤í”„ë¼ì¸: ì„¸íŠ¸/íƒ€ì„ì„¸ì¼/ì½œë¼ë³´ êµ¬ì²´ì•ˆ. (ì¶œì²˜: â€¦)

4. ê°€ê²©Â·êµ¬ì„± ì œì•ˆ(ì„ íƒ)
- â— ì ì‹¬ íšŒì „/ì €ë… ì²´ë¥˜í˜• ê° 1ì•ˆì”©: êµ¬ì„±/ê°€ê²©/ì „í™˜ íŠ¸ë¦¬ê±°. (ì¶œì²˜: â€¦)

5. ê·¼ê±°/ì¶œì²˜ ëª©ë¡
- ì‹ í•œì¹´ë“œë¶„ì„.jsonl#INS-â€¦, RULE-â€¦, POPUP-â€¦
- ì„±ìˆ˜ íŒì—… ìµœì¢….csv#rowâ€¦, ì„±ë™êµ¬ ê³µí†µ_í•œì–‘ëŒ€_í¥í–‰ì˜í™” ì´ë²¤íŠ¸ DB.csv#rowâ€¦"""

        # L0 í”„ë¡œí•„ ë¡œë”©
        profile_text = load_l0_profile(location, industry)
        
        # L1 ìŠ¬ë¦¼ RAG ê²€ìƒ‰ (í•„ìš”ì‹œì—ë§Œ)
        snippets = ""
        if needs_more_context(user_message, profile_text):
            snippets = slim_search(user_message, f"{location}:{industry}", 3)
        
        # í”„ë¡¬í”„íŠ¸ ì¡°ë¦½
        prompt_parts = [system_prompt]
        
        if profile_text:
            prompt_parts.append(f"[í”„ë¡œí•„]\n{profile_text}")
        
        if snippets:
            prompt_parts.append(f"[ë³´ê°•]\n{snippets}")
        
        prompt_parts.append(f"[ì‚¬ìš©ì ì§ˆë¬¸]\n{user_message}")
        
        full_prompt = "\n\n".join(prompt_parts)
        
        # ìºì‹œ í™•ì¸ (ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ì— ëŒ€í•œ ë¹ ë¥¸ ì‘ë‹µ)
        cache_key = user_message.lower().strip()
        if cache_key in response_cache:
            print(f"ğŸš€ ìºì‹œì—ì„œ ì‘ë‹µ ë°˜í™˜: {cache_key}")
            return jsonify({
                'message': response_cache[cache_key],
                'session_id': session_id
            })
        
        # RAG ê¸°ë°˜ ì‘ë‹µ (ëª¨ë“  ì§ˆë¬¸ì— ëŒ€í•´)
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        import time
        start_time = time.time()
        print(f"ğŸ” ê²€ìƒ‰ ì‹œì‘: {user_message[:30]}...")
        
        # Gemini API í˜¸ì¶œ (RAG ì»¨í…ìŠ¤íŠ¸ í¬í•¨) - íƒ€ì„ì•„ì›ƒ ì„¤ì •
        try:
            # ì§€ì—° ë¡œë”©ìœ¼ë¡œ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
            direct_model = get_model()
            if direct_model is None:
                print("âŒ API Key ë˜ëŠ” ëª¨ë¸ì´ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
                return jsonify({
                    'message': 'âŒ Google API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.',
                    'session_id': session_id
                }), 500
            
            # í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ì œí•œ (1200~1800 í† í° ê¸°ì¤€, ì•½ 4000~6000ì)
            if len(full_prompt) > 6000:
                # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë§Œ ì‚¬ìš©í•˜ì—¬ ê¸¸ì´ ì œí•œ
                full_prompt = f"{system_prompt}\n\n[ì‚¬ìš©ì ì§ˆë¬¸]\n{user_message}"
                print("âš ï¸ í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ê¸¸ì–´ì„œ RAG ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œì™¸í–ˆìŠµë‹ˆë‹¤.")
            
            # íƒ€ì„ì•„ì›ƒ ì„¤ì • (30ì´ˆ) - threading ë°©ì‹
            import threading
            import queue
            
            result_queue = queue.Queue()
            
            def api_call():
                try:
                    # ì™¸ë¶€ API í˜¸ì¶œì— íƒ€ì„ì•„ì›ƒ ë° ì¬ì‹œë„ ì¶”ê°€
                    import time
                    max_retries = 3
                    retry_delay = 2
                    
                    for attempt in range(max_retries):
                        try:
                            response = direct_model.generate_content(full_prompt)
                            result_queue.put(('success', response.text))
                            return
                        except Exception as e:
                            if attempt < max_retries - 1:
                                print(f"âš ï¸ API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {str(e)}")
                                time.sleep(retry_delay * (attempt + 1))  # ì§€ìˆ˜ ë°±ì˜¤í”„
                            else:
                                result_queue.put(('error', str(e)))
                except Exception as e:
                    result_queue.put(('error', str(e)))
            
            # API í˜¸ì¶œì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
            api_thread = threading.Thread(target=api_call)
            api_thread.daemon = True
            api_thread.start()
            
            # 180ì´ˆ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ê²°ê³¼ ëŒ€ê¸° (3ë¶„)
            try:
                result_type, result_data = result_queue.get(timeout=180)
                if result_type == 'success':
                    response_text = result_data
                    print(f"âœ… Gemini API ì‘ë‹µ ì„±ê³µ: {response_text[:50]}...")
                    print(f"ğŸ“Š í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(full_prompt)}ì")
                    print(f"â±ï¸ ì‘ë‹µ ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")
                else:
                    raise Exception(result_data)
            except queue.Empty:
                print("â° API íƒ€ì„ì•„ì›ƒ (120ì´ˆ ì´ˆê³¼)")
                response_text = """## â° ì‘ë‹µ ì‹œê°„ ì´ˆê³¼

ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ì´ ë„ˆë¬´ ë³µì¡í•´ì„œ ì²˜ë¦¬ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ê³  ìˆìŠµë‹ˆë‹¤.

### ğŸ”§ í•´ê²° ë°©ë²•:
1. **ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ** ë§ì”€í•´ì£¼ì„¸ìš”
2. **í‚¤ì›Œë“œ ì¤‘ì‹¬ìœ¼ë¡œ** ê°„ë‹¨íˆ ì§ˆë¬¸í•´ì£¼ì„¸ìš”
3. **ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„**í•´ì£¼ì„¸ìš”

### ğŸ’¡ ë¹ ë¥¸ ì§ˆë¬¸ ì˜ˆì‹œ:
- "ì„±ë™êµ¬ íŒì—… ì•Œë ¤ì¤˜"
- "ë§ˆì¼€íŒ… ì „ëµ ì¶”ì²œí•´ì¤˜"
- "ê³ ê° ìœ ì¹˜ ë°©ë²• ì•Œë ¤ì¤˜"

ë” ê°„ë‹¨í•œ ì§ˆë¬¸ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”! ğŸš€"""
                
        except TimeoutError:
            print("â° API íƒ€ì„ì•„ì›ƒ (180ì´ˆ ì´ˆê³¼)")
            response_text = """## â° ì‘ë‹µ ì‹œê°„ ì´ˆê³¼

ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ ì²˜ë¦¬ê°€ ì˜ˆìƒë³´ë‹¤ ì˜¤ë˜ ê±¸ë¦¬ê³  ìˆìŠµë‹ˆë‹¤.

### ğŸ”§ í•´ê²° ë°©ë²•:
1. **ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„**í•´ì£¼ì„¸ìš” (ì„œë²„ê°€ ë°”ì  ìˆ˜ ìˆìŠµë‹ˆë‹¤)
2. **ì§ˆë¬¸ì„ ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ** ë§ì”€í•´ì£¼ì„¸ìš”
3. **í‚¤ì›Œë“œ ì¤‘ì‹¬ìœ¼ë¡œ** ê°„ë‹¨íˆ ì§ˆë¬¸í•´ì£¼ì„¸ìš”

### ğŸ’¡ ë¹ ë¥¸ ì§ˆë¬¸ ì˜ˆì‹œ:
- "ì„±ë™êµ¬ íŒì—… ì•Œë ¤ì¤˜"
- "ë§ˆì¼€íŒ… ì „ëµ ì¶”ì²œí•´ì¤˜"
- "ê³ ê° ìœ ì¹˜ ë°©ë²• ì•Œë ¤ì¤˜"

ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”! ğŸš€"""
        except Exception as api_error:
            print(f"âŒ API Error: {str(api_error)}")
            
            # Google API ê´€ë ¨ ì˜¤ë¥˜ ì²˜ë¦¬
            if "API_KEY" in str(api_error) or "authentication" in str(api_error).lower():
                response_text = """## ğŸ”‘ API ì¸ì¦ ì˜¤ë¥˜

Google API Keyê°€ ì˜¬ë°”ë¥´ì§€ ì•Šê±°ë‚˜ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.

### ğŸ”§ í•´ê²° ë°©ë²•:
1. **Render Dashboard** â†’ **Environment**ì—ì„œ `GOOGLE_API_KEY` í™•ì¸
2. **Google AI Studio**ì—ì„œ ìƒˆë¡œìš´ API Key ìƒì„±
3. **ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜**í•˜ì„¸ìš”

### ğŸ“ API Key ì„¤ì • ë°©ë²•:
1. [Google AI Studio](https://makersuite.google.com/app/apikey) ì ‘ì†
2. **Create API Key** í´ë¦­
3. ìƒì„±ëœ í‚¤ë¥¼ Render Environmentì— ì¶”ê°€
"""
            elif "quota" in str(api_error).lower() or "limit" in str(api_error).lower():
                response_text = """## ğŸ“Š API í• ë‹¹ëŸ‰ ì´ˆê³¼

Google API ì‚¬ìš©ëŸ‰ì´ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.

### ğŸ”§ í•´ê²° ë°©ë²•:
1. **ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„**í•´ì£¼ì„¸ìš” (ë³´í†µ 1ì‹œê°„ í›„ ë³µêµ¬)
2. **ë” ê°„ë‹¨í•œ ì§ˆë¬¸**ìœ¼ë¡œ ì‹œë„í•´ì£¼ì„¸ìš”
3. **ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜**í•˜ì„¸ìš”

### ğŸ’¡ ë¹ ë¥¸ ì§ˆë¬¸ ì˜ˆì‹œ:
- "ì„±ë™êµ¬ íŒì—… ì•Œë ¤ì¤˜"
- "ë§ˆì¼€íŒ… ì „ëµ ì¶”ì²œí•´ì¤˜"
"""
            else:
                response_text = f"""## âŒ ì¼ì‹œì ì¸ ì˜¤ë¥˜ ë°œìƒ

ì£„ì†¡í•©ë‹ˆë‹¤. ì‹œìŠ¤í…œì— ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

### ğŸ”§ í•´ê²° ë°©ë²•:
1. **ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„**í•´ì£¼ì„¸ìš”
2. **ë” ê°„ë‹¨í•œ ì§ˆë¬¸**ìœ¼ë¡œ ì‹œë„í•´ì£¼ì„¸ìš”
3. **ë¬¸ì œê°€ ì§€ì†ë˜ë©´ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜**í•˜ì„¸ìš”

### ğŸ“ ì˜¤ë¥˜ ì •ë³´:
```
{str(api_error)}
```

ë” ê°„ë‹¨í•œ ì§ˆë¬¸ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”! ğŸš€"""
        
        # response_textê°€ ì´ë¯¸ ì„¤ì •ë˜ì–´ ìˆìŒ (í…ŒìŠ¤íŠ¸ ì‘ë‹µ ë˜ëŠ” API ì‘ë‹µ)
        
        # ì‘ë‹µ ìºì‹œì— ì €ì¥ (ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ìš©)
        cache_key = user_message.lower().strip()
        if len(cache_key) < 100:  # ë„ˆë¬´ ê¸´ ì§ˆë¬¸ì€ ìºì‹œí•˜ì§€ ì•ŠìŒ
            response_cache[cache_key] = response_text
            print(f"ğŸ’¾ ìºì‹œì— ì €ì¥: {cache_key[:20]}...")
        
        # ë§ˆí¬ë‹¤ìš´ì„ HTMLë¡œ ë³€í™˜
        html_response = markdown.markdown(response_text, extensions=['extra'])
        
        return jsonify({
            'message': html_response,
            'session_id': session_id
        })
    
    except Exception as e:
        import traceback
        print(f"Error: {str(e)}")
        print(f"Traceback: {traceback.format_exc()}")
        return jsonify({'error': f'ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'}), 500

@app.route('/api/test-gemini', methods=['GET'])
def test_gemini():
    """Gemini API ì§ì ‘ í…ŒìŠ¤íŠ¸"""
    try:
        direct_model = genai.GenerativeModel('gemini-2.5-flash')
        response = direct_model.generate_content('ì•ˆë…•í•˜ì„¸ìš”, ì„±ë™êµ¬ ì†Œìƒê³µì¸ ë§ˆì¼€íŒ… ë„ìš°ë¯¸ì…ë‹ˆë‹¤.')
        response_text = response.text
        html_response = markdown.markdown(response_text, extensions=['extra'])
        return jsonify({'response': html_response, 'status': 'success'})
    except Exception as e:
        return jsonify({'error': str(e), 'status': 'error'})

@app.route('/api/reset', methods=['POST'])
def reset():
    try:
        data = request.json
        session_id = data.get('session_id', 'default')
        
        if session_id in chat_sessions:
            del chat_sessions[session_id]
        
        return jsonify({'message': 'ëŒ€í™”ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.'})
    
    except Exception as e:
        return jsonify({'error': f'ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'}), 500

@app.route('/health', methods=['GET'])
def health_check():
    """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return jsonify({'status': 'ok'}), 200

# ====== ì„¤ì • ê´€ë¦¬ ì‹œìŠ¤í…œ ======

# ì „ì—­ ì„¤ì • ì €ì¥ì†Œ (ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ë‚˜ ì„¸ì…˜ì— ì €ì¥í•´ì•¼ í•¨)
user_setups = {}
preloaded_documents = []

@app.route('/api/check-setup', methods=['GET'])
def check_setup():
    """ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸"""
    try:
        # ì„¸ì…˜ ê¸°ë°˜ìœ¼ë¡œ ì„¤ì • í™•ì¸ (ì‹¤ì œë¡œëŠ” ë” ì•ˆì „í•œ ë°©ë²• ì‚¬ìš©)
        setup_info = user_setups.get('default', None)
        
        if setup_info:
            return jsonify({
                'is_setup': True,
                'setup_info': setup_info
            })
        else:
            return jsonify({
                'is_setup': False
            })
    except Exception as e:
        return jsonify({
            'is_setup': False,
            'error': str(e)
        }), 500

@app.route('/api/setup', methods=['POST'])
def save_setup():
    """ì„¤ì • ì •ë³´ ì €ì¥ ë° ë¬¸ì„œ ë¯¸ë¦¬ ë¡œë”©"""
    try:
        data = request.json
        location = data.get('location', '')
        industry = data.get('industry', '')
        store_name = data.get('store_name', '')
        
        if not location or not industry or not store_name:
            return jsonify({'error': 'ëª¨ë“  ì •ë³´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.'}), 400
        
        # ì„¤ì • ì •ë³´ ì €ì¥
        setup_info = {
            'location': location,
            'industry': industry,
            'store_name': store_name,
            'created_at': datetime.now().isoformat()
        }
        
        user_setups['default'] = setup_info
        
        # L0 í”„ë¡œí•„ ë¯¸ë¦¬ ë¡œë”©
        try:
            profile_text = load_l0_profile(location, industry)
            print(f"ğŸ“‹ L0 í”„ë¡œí•„ ë¯¸ë¦¬ ë¡œë”© ì™„ë£Œ: {location}_{industry}")
        except Exception as e:
            print(f"âš ï¸ L0 í”„ë¡œí•„ ë¡œë”© ì‹¤íŒ¨: {e}")
        
        # ê´€ë ¨ ë¬¸ì„œ ë¯¸ë¦¬ ë¡œë”© (ì‹ í•œì¹´ë“œ ë°ì´í„° ë“±)
        try:
            preload_relevant_documents(location, industry)
            print(f"ğŸ“š ê´€ë ¨ ë¬¸ì„œ ë¯¸ë¦¬ ë¡œë”© ì™„ë£Œ: {location}_{industry}")
        except Exception as e:
            print(f"âš ï¸ ë¬¸ì„œ ë¯¸ë¦¬ ë¡œë”© ì‹¤íŒ¨: {e}")
        
        return jsonify({
            'success': True,
            'message': 'ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.',
            'setup_info': setup_info
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/get-setup', methods=['GET'])
def get_setup():
    """ì €ì¥ëœ ì„¤ì • ì •ë³´ ì¡°íšŒ"""
    try:
        setup_info = user_setups.get('default', None)
        
        if setup_info:
            return jsonify({
                'success': True,
                'setup_info': setup_info
            })
        else:
            return jsonify({
                'success': False,
                'error': 'ì„¤ì • ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.'
            }), 404
            
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

def preload_relevant_documents(location, industry):
    """ê´€ë ¨ ë¬¸ì„œ ë¯¸ë¦¬ ë¡œë”©"""
    try:
        # ì‹ í•œì¹´ë“œ ë°ì´í„°ì—ì„œ ê´€ë ¨ ìŠ¤ë‹ˆí« ë¯¸ë¦¬ ê²€ìƒ‰
        shinhan_file = "documents/raw/ì‹ í•œì¹´ë“œë¶„ì„.jsonl"
        if os.path.exists(shinhan_file):
            relevant_docs = []
            with open(shinhan_file, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        data = json.loads(line)
                        content = data.get('body', '')
                        
                        # ì§€ì—­ ë° ì—…ì¢… ê´€ë ¨ì„± ì²´í¬
                        if (location and location in content) or (industry and industry in content):
                            relevant_docs.append({
                                'content': content,
                                'filename': 'ì‹ í•œì¹´ë“œë¶„ì„.jsonl',
                                'location': location,
                                'industry': industry
                            })
                    except:
                        continue
            
            # ì „ì—­ ë³€ìˆ˜ì— ì €ì¥ (ì‹¤ì œë¡œëŠ” ìºì‹œë‚˜ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥)
            global preloaded_documents
            preloaded_documents = relevant_docs
            print(f"ğŸ“š ë¯¸ë¦¬ ë¡œë”©ëœ ë¬¸ì„œ: {len(relevant_docs)}ê°œ")
            
    except Exception as e:
        print(f"âš ï¸ ë¬¸ì„œ ë¯¸ë¦¬ ë¡œë”© ì‹¤íŒ¨: {e}")

# ====== L0 í”„ë¡œí•„ ë° L1 ìŠ¬ë¦¼ RAG ì‹œìŠ¤í…œ ======

def load_l0_profile(location, industry):
    """L0 í”„ë¡œí•„ ë¬¸ì„œ ë¡œë”© (ì§€ì—­Ã—ì—…ì¢…ë³„ ë§ì¶¤ í”„ë¡œí•„)"""
    try:
        # í”„ë¡œí•„ íŒŒì¼ ê²½ë¡œ ìƒì„±
        profile_path = f"documents/profiles/{location}_{industry}.md"
        
        if os.path.exists(profile_path):
            with open(profile_path, 'r', encoding='utf-8') as f:
                content = f.read()
            print(f"ğŸ“‹ L0 í”„ë¡œí•„ ë¡œë“œ: {location}_{industry}")
            return content
        
        # ê¸°ë³¸ í”„ë¡œí•„ ìƒì„± (íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°)
        default_profile = generate_default_profile(location, industry)
        print(f"ğŸ“‹ ê¸°ë³¸ L0 í”„ë¡œí•„ ìƒì„±: {location}_{industry}")
        return default_profile
        
    except Exception as e:
        print(f"âš ï¸ L0 í”„ë¡œí•„ ë¡œë”© ì‹¤íŒ¨: {e}")
        return generate_default_profile(location, industry)

def generate_default_profile(location, industry):
    """ê¸°ë³¸ L0 í”„ë¡œí•„ ìƒì„±"""
    return f"""# {location} ì§€ì—­ {industry} ì—…ì¢… í”„ë¡œí•„

## ì§€ì—­ íŠ¹ì„±
- {location} ì§€ì—­ì˜ ìƒê¶Œ íŠ¹ì„± ë° ê³ ê°ì¸µ ë¶„ì„
- ì£¼ë³€ ê²½ìŸì—…ì²´ í˜„í™©
- ì ‘ê·¼ì„± ë° êµí†µí¸

## ì—…ì¢…ë³„ ì¸ì‚¬ì´íŠ¸
- {industry} ì—…ì¢…ì˜ {location} ì§€ì—­ ì í•©ë„
- íƒ€ê²Ÿ ê³ ê°ì¸µ íŠ¹ì„±
- ì„±ê³µ ì‚¬ë¡€ ë° ì‹¤íŒ¨ ìš”ì¸

## ë§ˆì¼€íŒ… ì „ëµ
- ì§€ì—­ ë§ì¶¤í˜• í™ë³´ ë°©ë²•
- ê³ ê° ìœ ì¹˜ ì „ëµ
- ê°€ê²© ì •ì±… ë° ì„œë¹„ìŠ¤ ê°œì„  ë°©ì•ˆ

## ì°¸ê³  ë°ì´í„°
- ì‹ í•œì¹´ë“œ ë°ì´í„° ê¸°ë°˜ ë¶„ì„ ê²°ê³¼
- ì§€ì—­ ì´ë²¤íŠ¸ ë° í”„ë¡œëª¨ì…˜ ì •ë³´"""

def needs_more_context(user_message, profile_text):
    """L1 ìŠ¬ë¦¼ RAG ë³´ê°•ì´ í•„ìš”í•œì§€ íŒë‹¨"""
    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ íŒë‹¨
    need_keywords = ['êµ¬ì²´ì ', 'ìƒì„¸í•œ', 'ìì„¸í•œ', 'ì„¸ë¶€', 'ë¶„ì„', 'ë°ì´í„°', 'í†µê³„', 'ë¹„êµ', 'ê²½ìŸì‚¬']
    return any(keyword in user_message for keyword in need_keywords)

def slim_search(user_message, partition, top_k=3):
    """L1 ìŠ¬ë¦¼ RAG ê²€ìƒ‰ (ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ê²€ìƒ‰)"""
    try:
        # íŒŒí‹°ì…˜ íŒŒì‹± (ì˜ˆ: "ì„±ìˆ˜:ì¹´í˜")
        if ':' in partition:
            region, industry = partition.split(':', 1)
        else:
            region, industry = "", ""
        
        # ê²€ìƒ‰ì–´ í™•ì¥
        search_terms = [region, industry, "íŒì—…", "ì´ë²¤íŠ¸", "ì†Œë¹„íŒ¨í„´", "ì‹œê°„ëŒ€", "ëŒ€í•™", "ì˜í™”", "ì„±ìˆ˜ê¸°"]
        search_query = " ".join([term for term in search_terms if term])
        
        relevant_snippets = []
        
        # 1ìˆœìœ„: ì‹ í•œì¹´ë“œë¶„ì„.jsonlì—ì„œ INS/RULE/POPUP/ìƒê¶ŒíŠ¹ì„± ê²€ìƒ‰
        shinhan_file = "documents/raw/ì‹ í•œì¹´ë“œë¶„ì„.jsonl"
        if os.path.exists(shinhan_file):
            with open(shinhan_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    try:
                        data = json.loads(line)
                        content = data.get('body', '')
                        doc_type = data.get('doc_type', '')
                        title = data.get('title', '')
                        
                        # INS, RULE, POPUP, ìƒê¶ŒíŠ¹ì„± í‚¤ì›Œë“œ ìš°ì„  ê²€ìƒ‰
                        if any(keyword in doc_type.lower() or keyword in title.lower() 
                               for keyword in ['ins', 'rule', 'popup', 'ìƒê¶ŒíŠ¹ì„±']):
                            if any(term in content.lower() for term in search_terms if term):
                                snippet = content[:600]  # ìœˆë„ìš° 400~600ì
                                source_tag = f"ì‹ í•œì¹´ë“œë¶„ì„.jsonl#{doc_type}-{line_num}"
                                relevant_snippets.append({
                                    'content': snippet,
                                    'source': source_tag,
                                    'priority': 1,
                                    'score': 1.0
                                })
                        
                        # ì¼ë°˜ì ì¸ ê´€ë ¨ì„± ì²´í¬ (ì‹ í•œì¹´ë“œ ë°ì´í„°)
                        relevance_score = 0
                        if region and region in content:
                            relevance_score += 2
                        if industry and industry in content:
                            relevance_score += 2
                        
                        if relevance_score > 0:
                            snippet = content[:600]
                            source_tag = f"ì‹ í•œì¹´ë“œë¶„ì„.jsonl#{doc_type}-{line_num}"
                            relevant_snippets.append({
                                'content': snippet,
                                'source': source_tag,
                                'priority': 1,
                                'score': relevance_score
                            })
                    except:
                        continue
        
        # 2ìˆœìœ„: CSV íŒŒì¼ë“¤ì—ì„œ ê²€ìƒ‰ (ìƒìœ„ 3ê°œë§Œ)
        csv_files = [
            ("ì„±ìˆ˜ íŒì—… ìµœì¢….csv", 2),
            ("ì„±ë™êµ¬ ê³µí†µ_í•œì–‘ëŒ€_í¥í–‰ì˜í™” ì´ë²¤íŠ¸ DB.csv", 2)
        ]
        
        for csv_file, priority in csv_files:
            csv_path = os.path.join(app.root_path, 'documents', 'raw', csv_file)
            if os.path.exists(csv_path):
                try:
                    df = pd.read_csv(csv_path)
                    for idx, row in df.iterrows():
                        row_content = " ".join([str(v) for v in row.values if pd.notna(v)])
                        if any(term in row_content.lower() for term in search_terms if term):
                            snippet = row_content[:600]
                            source_tag = f"{csv_file}#row{idx+1}"
                            relevant_snippets.append({
                                'content': snippet,
                                'source': source_tag,
                                'priority': priority,
                                'score': 0.8
                            })
                except Exception as e:
                    print(f"âš ï¸ CSV íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ {csv_file}: {e}")
        
        # ìš°ì„ ìˆœìœ„ì™€ ì ìˆ˜ë¡œ ì •ë ¬ (1ìˆœìœ„ ì‹ í•œì¹´ë“œ > 2ìˆœìœ„ CSV)
        relevant_snippets.sort(key=lambda x: (x['priority'], x['score']), reverse=True)
        
        # ì¤‘ë³µ ì œê±° (ìœ ì‚¬ë„ 0.9 ì´ìƒ)
        unique_snippets = []
        for snippet in relevant_snippets:
            is_duplicate = False
            for existing in unique_snippets:
                if len(set(snippet['content'].split()) & set(existing['content'].split())) / len(set(snippet['content'].split()) | set(existing['content'].split())) > 0.9:
                    is_duplicate = True
                    break
            if not is_duplicate:
                unique_snippets.append(snippet)
        
        # Top-K ì„ íƒ
        selected_snippets = unique_snippets[:top_k]
        
        # ìŠ¤ë‹ˆí« í¬ë§·íŒ…
        if selected_snippets:
            formatted_snippets = []
            for i, snippet in enumerate(selected_snippets, 1):
                formatted_snippets.append(f"[ìŠ¤ë‹ˆí« {i}] (ì¶œì²˜: {snippet['source']})\n{snippet['content']}")
            
            result = "\n\n".join(formatted_snippets)
            print(f"ğŸ” L1 ìŠ¬ë¦¼ RAG ê²€ìƒ‰ ì™„ë£Œ: {len(selected_snippets)}ê°œ ìŠ¤ë‹ˆí« (ìš°ì„ ìˆœìœ„: {[s['priority'] for s in selected_snippets]})")
            return result
        
        return ""
        
    except Exception as e:
        print(f"âš ï¸ L1 ìŠ¬ë¦¼ RAG ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return ""

@app.route('/api/calendar-events', methods=['GET'])
def get_calendar_events():
    """ë‹¬ë ¥ ì´ë²¤íŠ¸ ë°ì´í„° ë°˜í™˜"""
    try:
        # CSV íŒŒì¼ ê²½ë¡œ
        csv_file_path = os.path.join(app.root_path, 'documents', 'raw', 'ì„±ìˆ˜ íŒì—… ìµœì¢….csv')
        
        if not os.path.exists(csv_file_path):
            return jsonify({'error': 'CSV íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.'}), 404
        
        events = []
        with open(csv_file_path, mode='r', encoding='utf-8') as file:
            reader = csv.DictReader(file)
            for row in reader:
                events.append(row)
        
        return jsonify({'events': events})
    except Exception as e:
        print(f"Error loading calendar events: {e}")
        return jsonify({'error': 'ì´ë²¤íŠ¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.', 'details': str(e)}), 500

if __name__ == '__main__':
    import os
    port = int(os.environ.get('PORT', 8080))
    print(f"ğŸš€ Flask ì„œë²„ ì‹œì‘: 0.0.0.0:{port}")
    print(f"ğŸ“Š í™˜ê²½ë³€ìˆ˜ PORT: {os.environ.get('PORT', 'Not set')}")
    app.run(debug=False, host='0.0.0.0', port=port)
else:
    # Renderì—ì„œ Gunicornìœ¼ë¡œ ì‹¤í–‰ë  ë•ŒëŠ” ì´ ë¶€ë¶„ì´ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ
    # Gunicornì´ ì§ì ‘ app ê°ì²´ë¥¼ importí•˜ì—¬ ì‚¬ìš©
    print("ğŸ”§ Gunicorn ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘...")
    
    # Render í™˜ê²½ì—ì„œ ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ì•± ì‹œì‘ ì‹œ ë¬¸ì„œ ë¡œë”© ë¹„í™œì„±í™”
    print("ğŸš€ Render í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘ - RAG ë¬¸ì„œëŠ” ìš”ì²­ ì‹œ ë¡œë”©ë©ë‹ˆë‹¤.")
    rag_documents = {}
    document_index = {'keywords': {}, 'categories': {}, 'entities': {}}
    pass

